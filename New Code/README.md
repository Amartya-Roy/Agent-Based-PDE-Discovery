# LLM4ED: Agentic PDE Discovery System

A **fully autonomous** 4-agent system for discovering partial differential equations (PDEs) from spatiotemporal data. The system uses Large Language Models (LLMs) and Vision-Language Models (VLMs) to identify governing equations without any hardcoded operator libraries.

## ğŸ—ï¸ Architecture

The system uses 4 specialized AI agents in a collaborative pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Differential        â”‚  Converts raw data â†’ derivatives + contour plots
â”‚     Observer            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Phenomenology       â”‚  VLM analyzes contour â†’ identifies physics patterns
â”‚     Extractor (VLM)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Governing Law       â”‚  LLM generates 10 candidate PDEs based on patterns
â”‚     Synthesizer (LLM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Equation Arbiter    â”‚  LLM writes Python code to fit & evaluate candidates
â”‚     (Engineer LLM)      â”‚  Selects best PDE using RÂ² + parsimony
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     [If RÂ² < 80%: Loop back to Agent 3]
```

## âœ… Fully Agentic Design

**No manual hardcoding:**
- âŒ No hardcoded operator/operand libraries
- âŒ No pre-defined equation templates in code
- âœ… LLM generates candidate equations dynamically
- âœ… LLM writes Python fitting code dynamically
- âœ… VLM analyzes visual patterns autonomously

The only guidance provided is a list of "common PDE forms to consider" in the prompt (KdV, Burgers, Heat, KS, Chafee-Infante, Fisher-KPP), which the agents can use or ignore based on data analysis.

---

## ğŸ“‹ Requirements

### Python Version
- Python 3.9+ recommended

### Dependencies
```bash
pip install numpy scipy matplotlib pyyaml pyautogen openai
```

### API Key (Required)
The system uses NVIDIA's API for LLM and VLM inference. You need an NVIDIA API key.

**Option 1: Environment Variable (Recommended)**
```bash
# macOS/Linux
export NVIDIA_API_KEY="nvapi-your-api-key-here"

# Windows (Command Prompt)
set NVIDIA_API_KEY=nvapi-your-api-key-here

# Windows (PowerShell)
$env:NVIDIA_API_KEY="nvapi-your-api-key-here"
```

**Option 2: In config file**
Edit `datasets_config.yaml` and set:
```yaml
llm_config:
  api_key: "nvapi-your-api-key-here"
```

---

## ğŸš€ Quick Start

### macOS / Linux

```bash
# 1. Navigate to the project folder
cd "/Users/amartyaroy/Downloads/New Code"

# 2. Create virtual environment (first time only)
python3 -m venv .venv

# 3. Activate virtual environment
source .venv/bin/activate

# 4. Install dependencies
pip install numpy scipy matplotlib pyyaml pyautogen openai

# 5. Set API key
export NVIDIA_API_KEY="nvapi-your-api-key-here"

# 6. Run PDE discovery
python -c "from pde_discovery_system import predict_pde; predict_pde('kdv_data')"
```

### Windows (Command Prompt)

```cmd
:: 1. Navigate to the project folder
cd "C:\path\to\New Code"

:: 2. Create virtual environment (first time only)
python -m venv .venv

:: 3. Activate virtual environment
.venv\Scripts\activate.bat

:: 4. Install dependencies
pip install numpy scipy matplotlib pyyaml pyautogen openai

:: 5. Set API key
set NVIDIA_API_KEY=nvapi-your-api-key-here

:: 6. Run PDE discovery
python -c "from pde_discovery_system import predict_pde; predict_pde('kdv_data')"
```

### Windows (PowerShell)

```powershell
# 1. Navigate to the project folder
cd "C:\path\to\New Code"

# 2. Create virtual environment (first time only)
python -m venv .venv

# 3. Activate virtual environment
.\.venv\Scripts\Activate.ps1

# 4. Install dependencies
pip install numpy scipy matplotlib pyyaml pyautogen openai

# 5. Set API key
$env:NVIDIA_API_KEY="nvapi-your-api-key-here"

# 6. Run PDE discovery
python -c "from pde_discovery_system import predict_pde; predict_pde('kdv_data')"
```

---

## ğŸ“‚ Available Datasets

The following datasets are pre-configured:

| Dataset Key | PDE Type | Description |
|-------------|----------|-------------|
| `kdv_data` | Korteweg-de Vries | Soliton waves |
| `burgers_data` | Burgers | Shock waves |
| `chafee_infante_data` | Chafee-Infante | Reaction-diffusion (cubic) |
| `KS_data` | Kuramoto-Sivashinsky | Chaotic dynamics |
| `fisher_data` | Fisher-KPP | Population dynamics |

### Run on existing datasets

```python
from pde_discovery_system import predict_pde

# Discover PDE for any dataset
result = predict_pde('kdv_data')           # KdV equation
result = predict_pde('burgers_data')       # Burgers equation
result = predict_pde('chafee_infante_data') # Chafee-Infante
result = predict_pde('KS_data')            # Kuramoto-Sivashinsky
result = predict_pde('fisher_data')        # Fisher-KPP
```

Or run all datasets:
```python
from pde_discovery_system import AgenticPDEDiscovery

discovery = AgenticPDEDiscovery('datasets_config.yaml')
results = discovery.run_discovery()  # Runs on all datasets
```

---

## ğŸ“Š Adding New Data

### Supported Formats
- `.mat` (MATLAB files) - requires scipy
- `.npy` (NumPy arrays)

### Data Requirements
Your data should contain:
1. **Solution field `u(x,t)`**: 2D array with shape `(nx, nt)` where:
   - Rows = spatial points
   - Columns = time points
2. **Spatial grid `x`**: 1D array of spatial coordinates
3. **Temporal grid `t`**: 1D array of time coordinates

### Option A: Add to config file

Edit `datasets_config.yaml`:

```yaml
datasets:
  # ... existing datasets ...
  
  my_new_data:
    name: "My Custom Dataset"
    description: "Description of my PDE data"
    data_files:
      solution: "Data/my_data.mat"
      contour_plot: "Contours/my_data_contour.png"
    mat_keys:
      u: "usol"    # Variable name containing solution in .mat file
      x: "x"       # Variable name for spatial grid
      t: "t"       # Variable name for time grid
    physics_domain: "general"
```

For `.npy` files:
```yaml
  my_npy_data:
    name: "My NPY Dataset"
    description: "Data stored as numpy arrays"
    data_files:
      solution: "Data/my_solution.npy"
      x_grid: "Data/my_x.npy"
      t_grid: "Data/my_t.npy"
      contour_plot: "Contours/my_npy_contour.png"
    file_type: "npy"
    physics_domain: "general"
```

Then run:
```python
from pde_discovery_system import predict_pde
result = predict_pde('my_new_data')
```

### Option B: Programmatic loading

```python
from pde_discovery_system import AgenticPDEDiscovery
import numpy as np

# Create discovery instance
discovery = AgenticPDEDiscovery('datasets_config.yaml')

# Manually add dataset
discovery.datasets['my_custom_data'] = {
    'u': my_solution_array,      # Shape: (nx, nt)
    'x': my_spatial_grid,        # Shape: (nx,)
    't': my_time_grid,           # Shape: (nt,)
    'config': {'name': 'My Custom'},
    'precomputed_derivs': {}     # Optional: pre-computed derivatives
}

# Run discovery
result = discovery.discover_pde('my_custom_data')
```

### Pre-computed Derivatives (Optional)

If your `.mat` file contains pre-computed derivatives, the system will automatically detect and use them:

```matlab
% In MATLAB, save derivatives along with solution:
save('my_data.mat', 'U', 'x', 't', 'U_t', 'U_x', 'U_xx');
```

The system looks for variables named `U_t`, `U_x`, `U_xx` in `.mat` files.

---

## ğŸ“ Project Structure

```
New Code/
â”œâ”€â”€ pde_discovery_system.py    # Main system (4-agent architecture)
â”œâ”€â”€ datasets_config.yaml       # Configuration file
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ Data/                      # Dataset files
â”‚   â”œâ”€â”€ Kdv.mat
â”‚   â”œâ”€â”€ burgers.mat
â”‚   â”œâ”€â”€ kuramoto_sivishinky.mat
â”‚   â”œâ”€â”€ fisher_nonlin_groundtruth.mat
â”‚   â”œâ”€â”€ chafee_infante_CI.npy
â”‚   â”œâ”€â”€ chafee_infante_x.npy
â”‚   â””â”€â”€ chafee_infante_t.npy
â”œâ”€â”€ Contours/                  # Generated contour plots
â”‚   â””â”€â”€ [dataset]_contour.png
â””â”€â”€ .venv/                     # Python virtual environment (optional)
```

---

## ğŸ”§ Configuration Options

Edit `datasets_config.yaml` to customize:

```yaml
# Discovery parameters
discovery_settings:
  acceptance_threshold: 0.80    # Minimum RÂ² to accept equation (0-1)
  max_candidate_rounds: 5       # Max retry rounds if below threshold
  subsample_factor: 4           # Data subsampling (not used in current version)
  max_terms: 3                  # Preferred max terms in equation

# LLM models
llm_config:
  primary_model: "qwen/qwen3-coder-480b-a35b-instruct"  # Text LLM
  vision_model: "microsoft/phi-4-multimodal-instruct"   # Vision LLM
  api_base: "https://integrate.api.nvidia.com/v1"
  temperature: 0.7
  max_tokens: 2000
```

---


